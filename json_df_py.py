# -*- coding: utf-8 -*-
"""json-df.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17zS5fVDg38Zlci2PTmSsnBa4PU0ixLsi
"""

#conect with google disk
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import os, sys
import glob

import json

# Commented out IPython magic to ensure Python compatibility.
#  %%time
path = '/content/drive/MyDrive/Colab Notebooks/DS_test/json/'

#for file in (glob.glob(os.path.join(path, "*.json"))):
#    data += [json.loads(line) for line in open(file, 'r')]

with open('/content/drive/MyDrive/Colab Notebooks/DS_test/json/part-00001-900ddb8f-b0f1-4a8f-9dba-8ff2cdd88b38-c000.json') as f:
    data = [json.loads(line) for line in f]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# df = pd.DataFrame()
# for file in (glob.glob(os.path.join(path, "*.json"))):
#     with open(file) as f:
#         data = [json.loads(line) for line in f]
#         df1 = pd.json_normalize(data)
#     df = pd.concat([df1, df], ignore_index=True)
# 
# df

df.shape

"""### №3 What is(are) the main time period(s) in the data?
##### datetime
We have 2 time period:
2019/03/22 - 2019/03/29
2020/03/20 - 2020/03/27
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import datetime
# df['epochMillis_new'] = (df['epochMillis'].apply(lambda x: datetime.datetime.fromtimestamp(x/1000).strftime('%Y-%m-%d %H:%M:%S.%f')))
# df['date'] = pd.to_datetime(df['epochMillis_new'].apply(lambda x: x.split(' ')[0]))
# df['time'] = df['epochMillis_new'].apply(lambda x: (x.split(' ')[1].split('.')[0]))
#

df.head(5)

df['epochMillis_new'] = pd.to_datetime(df['epochMillis_new'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #df['time'] = pd.to_datetime(df['time']).apply(lambda x: x.strftime('%H:%M:%S.%f'))

df.dtypes

df

data_time = pd.DataFrame({'date': df['date'].value_counts().argsort(axis='index').index, 
                          'col': df['date'].value_counts().values}, 
                         index = df['date'].value_counts().argsort(axis='index') )
data_time['date'].sort_values(ascending=False)

"""# Answer:

#### We have 2 time period: 
<li>  2019/03/22 - 2019/03/29 </li>
<li>  2020/03/20 - 2020/03/27</li>

# №4 Which are the top three most sparse variables?
"""

df.fillna(0, inplace=True)
pd.DataFrame({'col': df.columns,
              'count': [(df[i]==0).sum()+((df[i]=='0')).sum() for i in df.columns]}).sort_values(by=['count']).tail(3)

"""# Answer:

The top three most sparse variables are navigation.rateOfTurn	(3361992) cargoDetails (3140605), imo (2646208).

# №5 What region(s) of the world and ocean port(s) does this data represent? Provide evidence to justify your answer.
"""

print('The region(s) of the world:', df['olson_timezone'].value_counts())
print('The ocean port:', df['port.name'].value_counts())

"""# Answer:
This data representThe the Asia/Shanghai region of the worl,
the SHANGHAI PT ocean port.

# №6 Provide a frequency tabulation of the various Navigation Codes & Descriptions (i.e., navCode & NavDesc). Optionally, provide any additional statistics you find interesting.

# Answer:
"""

pd.crosstab(df['navigation.navCode'], df['navigation.navDesc'])

print("df['navigation.navDesc']& df['navigation.navCode'].mean():",
  pd.crosstab( df['navigation.navDesc'], df['navigation.navCode']).mean())
print("df['navigation.navDesc']& df['navigation.navCode'].sum():",
  pd.crosstab( df['navigation.navDesc'], df['navigation.navCode']).sum())

#additional interesting statistics :
df.pivot_table(['navigation.navCode'], ['navigation.navDesc'])
'''or'''
print("df['navigation.navCode']& df['navigation.navDesc'].max:", 
      df.pivot_table(['navigation.navCode'], ['navigation.navDesc'], aggfunc=(np.max)).sort_values(['navigation.navCode']))

navigation = (pd.crosstab(df['navigation.navCode'], df['navigation.navDesc']).sum()).sort_values( ascending=False)
navigation

pd.crosstab(df['navigation.navCode'], df['navigation.navDesc']).sum()
#df.pivot_table(['navigation.navCode'], ['navigation.navDesc'])

"""#  №7 For MMSI = 205792000, provide the following report:
<ol><li> Limit the data to only the TOP 5 Navigation Codes based from the response to question 6 </li>
<li>Provide the final state for each series of contiguous events with the same Navigation Code; series may be interrupted by other </li>
<li>series, but each contiguous series must be its own record.</li> <ol>
Final report should include at least the following fields/columns:
<li>mmsi = the MMSI of the vessel 
timestamp = the timestamp of the last event in that contiguous series Navigation </li>
<li>Code = the navigation code (i.e., navigation.navCode) Navigation Description = the navigation code description (i.e., navigation.navDesc) </li>
<li>lead time (in Milliseconds) = the time difference in milliseconds between the last and first timestamp of that particular series of the same contiguous navigation codes</li>


"""

#df.sort_values(by=['hours-per-week', 'salary'], ascending=[False, True])

mmsi_205792000 = df.loc[ (df['mmsi'] == 205792000) &
       ((df['navigation.navDesc'] == navigation.index[0]) | 
       ( df['navigation.navDesc'] == navigation.index[1]) |  
       ( df['navigation.navDesc'] == navigation.index[2]) | 
       ( df['navigation.navDesc'] == navigation.index[3]) |  
       ( df['navigation.navDesc'] == navigation.index[4])), :].copy()

#df.loc[(df['mmsi'] == 205792000) , : ]['navigation.navDesc'].value_counts() # all  ['navigation.navDesc'] in top-5
mmsi_205792000

import time
#time.mktime(mmsi_205792000['epochMillis_new'].timetuple())
#df.a.dt.microsecond
mmsi_205792000['diff'] = (mmsi_205792000.iloc[:,27].diff())
#to ms
mmsi_205792000['diff'] = mmsi_205792000['diff'].apply(lambda x: x.total_seconds() * 1000.0)

#mmsi_205792000_time = (mmsi_205792000['epochMillis'].sort_values().tail(1).values[0])-(mmsi_205792000['epochMillis'].sort_values().head(1).values[0])
#print("lead time (in Milliseconds): ", (mmsi_205792000_time/10**6))

"""## Answer:"""

mmsi_205792000.loc[:,['mmsi','navigation.navCode', 'navigation.navDesc','date', 'time', 'diff']]

"""# 8 For MMSI = 413970021, provide the same report as number 7
<ol>Do you agree with the Navigation Code(s) and Description(s) for this particular vessel?</ol>
<li>If you do agree, provide an explanation why you agree.</li>
<li>If you do not agree, provide an explanation why do disagree.</li>
<li>Additionally, if you do not agree, what would you change it to and why? </li>
"""

mmsi_413970021 = df.loc[ (df['mmsi'] == 413970021) &
       ((df['navigation.navDesc'] == navigation.index[0]) | 
       ( df['navigation.navDesc'] == navigation.index[1]) |  
       ( df['navigation.navDesc'] == navigation.index[2]) | 
       ( df['navigation.navDesc'] == navigation.index[3]) |  
       ( df['navigation.navDesc'] == navigation.index[4])), :].copy()

#df.loc[(df['mmsi'] == 413970021) , : ]['navigation.navDesc'].value_counts() # 'Unknown'  ['navigation.navDesc'] in top-5
mmsi_413970021

#time.mktime(mmsi_413970021['epochMillis_new'].timetuple())

mmsi_413970021['diff'] = (mmsi_413970021.iloc[:,27].diff())
#to ms
mmsi_413970021['diff'] = mmsi_413970021['diff'].apply(lambda x: x.total_seconds() * 1000.0)

"""# Answer:
I agree, this ia a particular vessel (Cargo	'XIANG HAO 08').
"""

mmsi_413970021.loc[:,['mmsi','navigation.navCode', 'navigation.navDesc','date', 'time', 'diff']]

mmsi_413970021.pivot_table(['vesselDetails.name'], ['vesselDetails.typeName'], aggfunc=(np.max)).sort_values(['vesselDetails.name'])

"""# 9 For each of the time period(s) from item three, provide a tabulation of the top 10 series of vessel navigation code/description ordered states.

"""

df

df['year'] = df['date'].apply(lambda x: x.strftime('%Y'))

period_1 = df.loc[(df['year'] == 2019),:].copy()
period_2 = df.loc[(df['year'] == 2020),:].copy()

#pd.to_datetime(2020, unit='year')
pd.to_datetime('2020', format='%Y')

"""# Answer:"""

print('period 1 (2019):')
period_2019_top10 = (pd.crosstab(period_1['navigation.navCode'], period_1['navigation.navDesc']).sum()).sort_values(ascending=False)[:10]
period_2019_top10

print('period 2 (2020):')
period_2020_top10 = pd.crosstab(period_2['navigation.navCode'], period_2['navigation.navDesc']).sum().sort_values(ascending=False)[:10]
period_2020_top10

"""# 10 Using the results from item 9, compare the volume of each vessel navigation code/description ordered states for each time period(s) from item three. 
<li>Which increased/decreased?</li>
"""

#period_2020_top10.describe()# 
for i in period_2019_top10.index:
    if i not in period_2020_top10.index:
        print(i, ' - decreased, not in top10 now')
    elif  period_2019_top10[i] < period_2020_top10[i]:
        print(i,' - increased')      
    else:
        print(i, ' - decreased')

"""# 11 For each of the time period(s) from item three and using only the “At Anchor” and “Moored” navigation descriptions, quantify the average “dwell”

# Answer:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # period 1:
# 
# period_1[(period_1['navigation.navDesc'] == 'At Anchor') | (period_1['navigation.navDesc'] == 'Moored')]['epochMillis_new'].diff()

#
period_2[(period_2['navigation.navDesc'] == 'At Anchor') | (period_2['navigation.navDesc'] == 'Moored')]['epochMillis_new'].diff()

"""# 12 Describe or show how you would quantify if the difference(s) in “dwells” between the time-period(s) is(are) significant."""

#period_1[(period_1['navigation.navDesc'] == 'At Anchor') | (period_1['navigation.navDesc'] == 'Moored')]['epochMillis_new'].diff()

dwells = ((data_time[data_time['date'] > pd.to_datetime('2020',
             format='%Y')]['date']).sort_values(ascending=False).tail(1).values) - ((data_time[data_time['date'] < pd.to_datetime('2020',  
             format='%Y')]['date']).sort_values(ascending=True).tail(1).values)

print( "Difference between the time-period(s) is", dwells[0])

"""# 13 Describe or show how you would create a Machine Learning Model to predict “dwell” times for the region.
Bonus Points: Provide the code and performance results for your machine learning model on an OOB sample.
In respect of time, performance of the ML model is not important and the number of features can be minimal. What mattes is
that the code works and you can explain your thought process if asked.

#Answer: https://www.researchgate.net/publication/304530252_Development_of_Models_Predicting_Dwell_Time_of_Import_Containers_in_Port_Container_Terminals_-_An_Artificial_Neural_Networks_Application
"""

